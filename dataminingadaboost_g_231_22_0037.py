# -*- coding: utf-8 -*-
"""DataMiningAdaboost_G.231.22.0037.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D7Tp1EAvE8jft5e4Oc6OdIft4yfCHsEv
"""

#Load libraries
from sklearn.ensemble import AdaBoostClassifier
from sklearn import datasets
# Import train_tes_split function
from sklearn.model_selection import train_test_split
#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics

# Load data
iris = datasets.load_iris()
x = iris.data
y = iris.target

# Split dataset into training set and test set
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3) # 70% training and 30% test

# Create adaboost clasifier object
abc = AdaBoostClassifier(n_estimators=50,learning_rate=1)
# Train Adaboost Clasifier
model = abc.fit(X_train, y_train)
#Predict the response for test dataset
y_pred = model.predict(X_test)

# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

!git clone https://github.com/eriklindernoren/ML-From-Scratch

import sys
sys.path.append('/content/ML-From-Scratch')

from __future__ import division, print_function
import numpy as np
import math
from sklearn import datasets
import matplotlib.pyplot as plt
import pandas as pd

# Import helper functions
from mlfromscratch.utils import train_test_split, accuracy_score, Plot

# Decision stump used as weak classifier in this impl. of Adaboost
class DecisionStump():
    def __init__(self):
        # Determines if sample shall be clasified as -1 or 1 given threshold
        self.polarity = 1
        # The index of the feature used to make classification
        self.feature_index = None
        # The thresold value that the feature should be measured against
        self.threshold = None
        # Value indicative of the classifier's accuracy
        self.alpha = None

class Adaboost ():
     """Boosting method that uses a number of weak classifiers in
    enesmble to make a strong classifier. This implementation uses decision
    stumps, wich is a one level Decision Tree.

    Parameters:
    -----------
    n_clf: int
        The number of weak classifiers that will be used.
    """
    def ___init___(self, n_clf=5):
        self.n_clf = n_clf

    def fit(self, x, y):
        n_samples, n_features = np.shape(x)